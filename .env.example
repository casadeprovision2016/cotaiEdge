# CotAi Edge Environment Configuration Template
# Copy this file to .env and fill in your actual values

# =============================================================================
# SUPABASE CONFIGURATION (Required)
# =============================================================================
# Your Supabase project URL
SUPABASE_URL=https://api.neuro-ia.es

# Supabase anonymous key (public key)
SUPABASE_ANON_KEY=your_anon_key_here

# Supabase service role key (private key - keep secure!)
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key_here

# =============================================================================
# SECURITY CONFIGURATION (Required)
# =============================================================================
# JWT secret key for token signing (generate a random strong key)
JWT_SECRET=your-super-secret-jwt-key-here-make-it-long-and-random

# =============================================================================
# AI SERVICE CONFIGURATION
# =============================================================================
# Service debugging and logging
DEBUG=false
LOG_LEVEL=INFO

# API server configuration
HOST=0.0.0.0
PORT=8000

# Docling AI model configuration
DOCLING_ARTIFACTS_PATH=./ai-service/models
DOCLING_ENABLE_REMOTE_SERVICES=false

# OCR engine configuration
OCR_ENGINE=easyocr
OCR_LANGUAGES=pt,en
OCR_USE_GPU=false

# Processing limits
MAX_FILE_SIZE=52428800
MAX_PAGES=1000
OMP_NUM_THREADS=4

# Quality assessment thresholds
QUALITY_THRESHOLD_EXCELLENT=0.9
QUALITY_THRESHOLD_GOOD=0.7
QUALITY_THRESHOLD_FAIR=0.5

# Storage paths
STORAGE_ROOT_PATH=./storage
TEMP_DIRECTORY_PATH=./temp
RESULTS_DIRECTORY_PATH=./results

# Cache configuration
REDIS_URL=redis://localhost:6379
CACHE_TTL=3600

# CORS configuration
ALLOWED_ORIGINS=*

# =============================================================================
# LLM CONFIGURATION (Optional - for advanced analysis)
# =============================================================================
# LLM model for advanced document analysis
LLM_MODEL=llama-3.2
LLM_ENDPOINT=http://localhost:11434
LLM_MAX_TOKENS=4096
LLM_API_KEY=

# =============================================================================
# MONITORING CONFIGURATION (Optional)
# =============================================================================
# Sentry DSN for error monitoring
SENTRY_DSN=

# =============================================================================
# CLOUDFLARE CONFIGURATION (Optional - for production)
# =============================================================================
# Cloudflare Tunnel token for secure access
CLOUDFLARE_TUNNEL_TOKEN=

# =============================================================================
# DEVELOPMENT CONFIGURATION
# =============================================================================
# Frontend URLs (for development)
NEXT_PUBLIC_AI_SERVICE_URL=http://localhost:8000
NEXT_PUBLIC_BACKEND_URL=http://localhost:3001
NEXT_PUBLIC_SUPABASE_URL=https://api.neuro-ia.es
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_anon_key_here

# Backend service configuration
NODE_ENV=production
AI_SERVICE_URL=http://ai-service:8000